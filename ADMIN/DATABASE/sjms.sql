# Host: localhost  (Version: 5.7.10-log)
# Date: 2016-07-21 19:20:52
# Generator: MySQL-Front 5.3  (Build 5.16)

/*!40101 SET NAMES utf8 */;

#
# Structure for table "admin"
#

DROP TABLE IF EXISTS `admin`;
CREATE TABLE `admin` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(64) DEFAULT NULL,
  `password` varchar(128) DEFAULT NULL,
  `photo` varchar(128) DEFAULT NULL,
  `email` varchar(64) DEFAULT NULL,
  `number` varchar(32) DEFAULT NULL,
  `login_time` varchar(64) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COMMENT='管理员表';

#
# Data for table "admin"
#

INSERT INTO `admin` VALUES (1,'汪江123','5F4DCC3B5AA765D61D8327DEB882CF99','admin_photo.jpg','1184829149@qq.com','0123456789123','2016/07/21 19:19:28');

#
# Structure for table "job"
#

DROP TABLE IF EXISTS `job`;
CREATE TABLE `job` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `jid` varchar(44) DEFAULT NULL,
  `uid` int(11) DEFAULT NULL,
  `jar_path` varchar(1024) DEFAULT NULL,
  `in_path` varchar(1024) DEFAULT NULL,
  `out_path` varchar(1024) DEFAULT NULL,
  `main_class` varchar(30) DEFAULT NULL COMMENT '主类名(新增任务时需要)',
  `memory` int(4) DEFAULT NULL COMMENT '内存数量(运行前需要)default=默认值',
  `cores` int(4) DEFAULT NULL COMMENT 'CPU核数同memory',
  `status` int(4) DEFAULT NULL,
  `log` text,
  `st` varchar(30) DEFAULT NULL,
  `et` varchar(30) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=127 DEFAULT CHARSET=utf8;

#
# Data for table "job"
#

INSERT INTO `job` VALUES (125,NULL,3,'/home/a.jar','/home/A/text.txt','/home/0021','test',2,3,0,NULL,NULL,NULL),(126,'app-20160721165544-0009',3,'/home/a.jar','/home/A/text.txt','/home/0021','test',2,3,0,NULL,'2016-07-21 16:55:43',NULL),(127,'app-20160721165703-0010',3,'/home/a.jar','/home/A/text.txt','/home/0021','test',2,3,0,NULL,'2016-07-21 16:57:01',NULL),(128,'app-20160721165749-0011',3,'/home/a.jar','/home/A/text.txt','/home/02','test',2,3,0,NULL,'2016-07-21 16:57:47',NULL),(129,'app-20160721165848-0012',3,'/home/a.jar','/home/A/text.txt','/home/11','test',2,3,1,'Using Spark\'s default log4j profile: org/apache/spark/log4j-defaults.properties\n16/07/21 16:58:46 INFO SparkContext: Running Spark version 1.4.0\n16/07/21 16:58:46 WARN Utils: Your hostname, linux resolves to a loopback address: 127.0.0.1; using 172.19.142.65 instead (on interface eno1)\n16/07/21 16:58:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n16/07/21 16:58:46 INFO SecurityManager: Changing view acls to: Hash.meng\n16/07/21 16:58:46 INFO SecurityManager: Changing modify acls to: Hash.meng\n16/07/21 16:58:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Hash.meng); users with modify permissions: Set(Hash.meng)\n16/07/21 16:58:47 INFO Slf4jLogger: Slf4jLogger started\n16/07/21 16:58:47 INFO Remoting: Starting remoting\n16/07/21 16:58:47 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.19.142.65:35229]\n16/07/21 16:58:47 INFO Utils: Successfully started service \'sparkDriver\' on port 35229.\n16/07/21 16:58:47 INFO SparkEnv: Registering MapOutputTracker\n16/07/21 16:58:47 INFO SparkEnv: Registering BlockManagerMaster\n16/07/21 16:58:47 INFO DiskBlockManager: Created local directory at /tmp/spark-9a95c300-3165-405e-bf63-bd3c6bc08cd9/blockmgr-0581acff-0d16-4cca-8c7f-02951b2fea96\n16/07/21 16:58:47 INFO MemoryStore: MemoryStore started with capacity 265.4 MB\n16/07/21 16:58:47 INFO HttpFileServer: HTTP File server directory is /tmp/spark-9a95c300-3165-405e-bf63-bd3c6bc08cd9/httpd-eb06ba79-f490-4ce4-a2da-726dc7156c8f\n16/07/21 16:58:47 INFO HttpServer: Starting HTTP Server\n16/07/21 16:58:47 INFO Utils: Successfully started service \'HTTP file server\' on port 41063.\n16/07/21 16:58:47 INFO SparkEnv: Registering OutputCommitCoordinator\n16/07/21 16:58:47 INFO Utils: Successfully started service \'SparkUI\' on port 4040.\n16/07/21 16:58:47 INFO SparkUI: Started SparkUI at http://172.19.142.65:4040\n16/07/21 16:58:47 INFO SparkContext: Added JAR file:/home/Hash.meng/IdeaProjects/javahdfs/web/LocalFile/null-a.jar at http://172.19.142.65:41063/jars/null-a.jar with timestamp 1469091527555\n16/07/21 16:58:47 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@172.19.142.65:7077/user/Master...\n16/07/21 16:58:48 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160721165848-0012\n16/07/21 16:58:48 INFO AppClient$ClientActor: Executor added: app-20160721165848-0012/0 on worker-20160720192555-172.19.142.65-36703 (172.19.142.65:36703) with 3 cores\n16/07/21 16:58:48 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160721165848-0012/0 on hostPort 172.19.142.65:36703 with 3 cores, 2.0 GB RAM\n16/07/21 16:58:48 INFO AppClient$ClientActor: Executor updated: app-20160721165848-0012/0 is now LOADING\n16/07/21 16:58:48 INFO AppClient$ClientActor: Executor updated: app-20160721165848-0012/0 is now RUNNING\n16/07/21 16:58:48 INFO Utils: Successfully started service \'org.apache.spark.network.netty.NettyBlockTransferService\' on port 39429.\n16/07/21 16:58:48 INFO NettyBlockTransferService: Server created on 39429\n16/07/21 16:58:48 INFO BlockManagerMaster: Trying to register BlockManager\n16/07/21 16:58:48 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.142.65:39429 with 265.4 MB RAM, BlockManagerId(driver, 172.19.142.65, 39429)\n16/07/21 16:58:48 INFO BlockManagerMaster: Registered BlockManager\n16/07/21 16:58:48 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n16/07/21 16:58:48 INFO MemoryStore: ensureFreeSpace(33392) called with curMem=0, maxMem=278302556\n16/07/21 16:58:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.6 KB, free 265.4 MB)\n16/07/21 16:58:48 INFO MemoryStore: ensureFreeSpace(3631) called with curMem=33392, maxMem=278302556\n16/07/21 16:58:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KB, free 265.4 MB)\n16/07/21 16:58:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.142.65:39429 (size: 3.5 KB, free: 265.4 MB)\n16/07/21 16:58:48 INFO SparkContext: Created broadcast 0 from textFile at test.scala:13\n16/07/21 16:58:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/07/21 16:58:48 WARN LoadSnappy: Snappy native library not loaded\n16/07/21 16:58:48 INFO FileInputFormat: Total input paths to process : 1\n16/07/21 16:58:49 INFO SparkContext: Starting job: saveAsTextFile at test.scala:14\n16/07/21 16:58:49 INFO DAGScheduler: Registering RDD 3 (map at test.scala:14)\n16/07/21 16:58:49 INFO DAGScheduler: Got job 0 (saveAsTextFile at test.scala:14) with 2 output partitions (allowLocal=false)\n16/07/21 16:58:49 INFO DAGScheduler: Final stage: ResultStage 1(saveAsTextFile at test.scala:14)\n16/07/21 16:58:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n16/07/21 16:58:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n16/07/21 16:58:49 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:14), which has no missing parents\n16/07/21 16:58:49 INFO MemoryStore: ensureFreeSpace(3984) called with curMem=37023, maxMem=278302556\n16/07/21 16:58:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 265.4 MB)\n16/07/21 16:58:49 INFO MemoryStore: ensureFreeSpace(2298) called with curMem=41007, maxMem=278302556\n16/07/21 16:58:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)\n16/07/21 16:58:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.142.65:39429 (size: 2.2 KB, free: 265.4 MB)\n16/07/21 16:58:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874\n16/07/21 16:58:49 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:14)\n16/07/21 16:58:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n16/07/21 16:58:49 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@172.19.142.65:37011/user/Executor#-39149994]) with ID 0\n16/07/21 16:58:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.19.142.65, PROCESS_LOCAL, 1452 bytes)\n16/07/21 16:58:49 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.19.142.65, PROCESS_LOCAL, 1452 bytes)\n16/07/21 16:58:49 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.142.65:43965 with 1060.3 MB RAM, BlockManagerId(0, 172.19.142.65, 43965)\n16/07/21 16:58:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.142.65:43965 (size: 2.2 KB, free: 1060.3 MB)\n16/07/21 16:58:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.142.65:43965 (size: 3.5 KB, free: 1060.3 MB)\n16/07/21 16:58:50 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1007 ms on 172.19.142.65 (1/2)\n16/07/21 16:58:50 INFO DAGScheduler: ShuffleMapStage 0 (map at test.scala:14) finished in 1.488 s\n16/07/21 16:58:50 INFO DAGScheduler: looking for newly runnable stages\n16/07/21 16:58:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1028 ms on 172.19.142.65 (2/2)\n16/07/21 16:58:50 INFO DAGScheduler: running: Set()\n16/07/21 16:58:50 INFO DAGScheduler: waiting: Set(ResultStage 1)\n16/07/21 16:58:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n16/07/21 16:58:50 INFO DAGScheduler: failed: Set()\n16/07/21 16:58:50 INFO DAGScheduler: Missing parents for ResultStage 1: List()\n16/07/21 16:58:50 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:14), which is now runnable\n16/07/21 16:58:50 INFO MemoryStore: ensureFreeSpace(21208) called with curMem=43305, maxMem=278302556\n16/07/21 16:58:50 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.7 KB, free 265.3 MB)\n16/07/21 16:58:50 INFO MemoryStore: ensureFreeSpace(7036) called with curMem=64513, maxMem=278302556\n16/07/21 16:58:50 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 265.3 MB)\n16/07/21 16:58:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.142.65:39429 (size: 6.9 KB, free: 265.4 MB)\n16/07/21 16:58:50 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874\n16/07/21 16:58:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:14)\n16/07/21 16:58:50 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n16/07/21 16:58:50 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.19.142.65, PROCESS_LOCAL, 1217 bytes)\n16/07/21 16:58:50 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.19.142.65, PROCESS_LOCAL, 1217 bytes)\n16/07/21 16:58:50 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.142.65:43965 (size: 6.9 KB, free: 1060.3 MB)\n16/07/21 16:58:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.142.65:37011\n16/07/21 16:58:50 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes\n16/07/21 16:58:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 391 ms on 172.19.142.65 (1/2)\n16/07/21 16:58:51 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 425 ms on 172.19.142.65 (2/2)\n16/07/21 16:58:51 INFO DAGScheduler: ResultStage 1 (saveAsTextFile at test.scala:14) finished in 0.427 s\n16/07/21 16:58:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n16/07/21 16:58:51 INFO DAGScheduler: Job 0 finished: saveAsTextFile at test.scala:14, took 1.974569 s\n16/07/21 16:58:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.142.65:39429 in memory (size: 2.2 KB, free: 265.4 MB)\n16/07/21 16:58:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.142.65:43965 in memory (size: 2.2 KB, free: 1060.3 MB)\n16/07/21 16:58:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.142.65:39429 in memory (size: 6.9 KB, free: 265.4 MB)\n16/07/21 16:58:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.142.65:43965 in memory (size: 6.9 KB, free: 1060.3 MB)\n16/07/21 16:58:51 INFO ContextCleaner: Cleaned shuffle 0\n16/07/21 16:58:51 INFO SparkUI: Stopped Spark web UI at http://172.19.142.65:4040\n16/07/21 16:58:51 INFO DAGScheduler: Stopping DAGScheduler\n16/07/21 16:58:51 INFO SparkDeploySchedulerBackend: Shutting down all executors\n16/07/21 16:58:51 INFO SparkDeploySchedulerBackend: Asking each executor to shut down\n16/07/21 16:58:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n16/07/21 16:58:51 INFO Utils: path = /tmp/spark-9a95c300-3165-405e-bf63-bd3c6bc08cd9/blockmgr-0581acff-0d16-4cca-8c7f-02951b2fea96, already present as root for deletion.\n16/07/21 16:58:51 INFO MemoryStore: MemoryStore cleared\n16/07/21 16:58:51 INFO BlockManager: BlockManager stopped\n16/07/21 16:58:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n16/07/21 16:58:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n16/07/21 16:58:51 INFO SparkContext: Successfully stopped SparkContext\n16/07/21 16:58:51 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n16/07/21 16:58:51 INFO Utils: Shutdown hook called\n16/07/21 16:58:51 INFO Utils: Deleting directory /tmp/spark-9a95c300-3165-405e-bf63-bd3c6bc08cd9\n16/07/21 16:58:51 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n16/07/21 16:58:51 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n','2016-07-21 16:58:46','2016-07-21 16:58:51'),(130,'app-20160721172004-0013',3,'/home/a.jar','/home/A/text.txt','/home/003','test',2,3,1,'Using Spark\'s default log4j profile: org/apache/spark/log4j-defaults.properties\n16/07/21 17:20:03 INFO SparkContext: Running Spark version 1.4.0\n16/07/21 17:20:03 WARN Utils: Your hostname, linux resolves to a loopback address: 127.0.0.1; using 172.19.142.65 instead (on interface eno1)\n16/07/21 17:20:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n16/07/21 17:20:03 INFO SecurityManager: Changing view acls to: Hash.meng\n16/07/21 17:20:03 INFO SecurityManager: Changing modify acls to: Hash.meng\n16/07/21 17:20:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Hash.meng); users with modify permissions: Set(Hash.meng)\n16/07/21 17:20:03 INFO Slf4jLogger: Slf4jLogger started\n16/07/21 17:20:03 INFO Remoting: Starting remoting\n16/07/21 17:20:03 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.19.142.65:39971]\n16/07/21 17:20:03 INFO Utils: Successfully started service \'sparkDriver\' on port 39971.\n16/07/21 17:20:03 INFO SparkEnv: Registering MapOutputTracker\n16/07/21 17:20:03 INFO SparkEnv: Registering BlockManagerMaster\n16/07/21 17:20:03 INFO DiskBlockManager: Created local directory at /tmp/spark-059b03ed-1a07-46ed-8020-8210b775fd88/blockmgr-86cf89c8-e6e1-4790-b7e9-b73689551a99\n16/07/21 17:20:03 INFO MemoryStore: MemoryStore started with capacity 265.4 MB\n16/07/21 17:20:03 INFO HttpFileServer: HTTP File server directory is /tmp/spark-059b03ed-1a07-46ed-8020-8210b775fd88/httpd-a596dce7-ae80-4428-a2ef-1d3a7bbd81f5\n16/07/21 17:20:03 INFO HttpServer: Starting HTTP Server\n16/07/21 17:20:03 INFO Utils: Successfully started service \'HTTP file server\' on port 40493.\n16/07/21 17:20:03 INFO SparkEnv: Registering OutputCommitCoordinator\n16/07/21 17:20:04 INFO Utils: Successfully started service \'SparkUI\' on port 4040.\n16/07/21 17:20:04 INFO SparkUI: Started SparkUI at http://172.19.142.65:4040\n16/07/21 17:20:04 INFO SparkContext: Added JAR file:/home/Hash.meng/IdeaProjects/javahdfs/web/LocalFile/null-a.jar at http://172.19.142.65:40493/jars/null-a.jar with timestamp 1469092804235\n16/07/21 17:20:04 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@172.19.142.65:7077/user/Master...\n16/07/21 17:20:04 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160721172004-0013\n16/07/21 17:20:04 INFO AppClient$ClientActor: Executor added: app-20160721172004-0013/0 on worker-20160720192555-172.19.142.65-36703 (172.19.142.65:36703) with 3 cores\n16/07/21 17:20:04 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160721172004-0013/0 on hostPort 172.19.142.65:36703 with 3 cores, 2.0 GB RAM\n16/07/21 17:20:04 INFO AppClient$ClientActor: Executor updated: app-20160721172004-0013/0 is now LOADING\n16/07/21 17:20:04 INFO AppClient$ClientActor: Executor updated: app-20160721172004-0013/0 is now RUNNING\n16/07/21 17:20:04 INFO Utils: Successfully started service \'org.apache.spark.network.netty.NettyBlockTransferService\' on port 35167.\n16/07/21 17:20:04 INFO NettyBlockTransferService: Server created on 35167\n16/07/21 17:20:04 INFO BlockManagerMaster: Trying to register BlockManager\n16/07/21 17:20:04 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.142.65:35167 with 265.4 MB RAM, BlockManagerId(driver, 172.19.142.65, 35167)\n16/07/21 17:20:04 INFO BlockManagerMaster: Registered BlockManager\n16/07/21 17:20:04 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n16/07/21 17:20:04 INFO MemoryStore: ensureFreeSpace(33392) called with curMem=0, maxMem=278302556\n16/07/21 17:20:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.6 KB, free 265.4 MB)\n16/07/21 17:20:04 INFO MemoryStore: ensureFreeSpace(3631) called with curMem=33392, maxMem=278302556\n16/07/21 17:20:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KB, free 265.4 MB)\n16/07/21 17:20:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.142.65:35167 (size: 3.5 KB, free: 265.4 MB)\n16/07/21 17:20:04 INFO SparkContext: Created broadcast 0 from textFile at test.scala:13\n16/07/21 17:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/07/21 17:20:04 WARN LoadSnappy: Snappy native library not loaded\n16/07/21 17:20:04 INFO FileInputFormat: Total input paths to process : 1\n16/07/21 17:20:05 INFO SparkContext: Starting job: saveAsTextFile at test.scala:14\n16/07/21 17:20:05 INFO DAGScheduler: Registering RDD 3 (map at test.scala:14)\n16/07/21 17:20:05 INFO DAGScheduler: Got job 0 (saveAsTextFile at test.scala:14) with 2 output partitions (allowLocal=false)\n16/07/21 17:20:05 INFO DAGScheduler: Final stage: ResultStage 1(saveAsTextFile at test.scala:14)\n16/07/21 17:20:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n16/07/21 17:20:05 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n16/07/21 17:20:05 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:14), which has no missing parents\n16/07/21 17:20:05 INFO MemoryStore: ensureFreeSpace(3984) called with curMem=37023, maxMem=278302556\n16/07/21 17:20:05 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 265.4 MB)\n16/07/21 17:20:05 INFO MemoryStore: ensureFreeSpace(2298) called with curMem=41007, maxMem=278302556\n16/07/21 17:20:05 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 265.4 MB)\n16/07/21 17:20:05 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.142.65:35167 (size: 2.2 KB, free: 265.4 MB)\n16/07/21 17:20:05 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:874\n16/07/21 17:20:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:14)\n16/07/21 17:20:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n16/07/21 17:20:05 INFO SparkDeploySchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@172.19.142.65:44229/user/Executor#-253738452]) with ID 0\n16/07/21 17:20:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 172.19.142.65, PROCESS_LOCAL, 1452 bytes)\n16/07/21 17:20:05 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 172.19.142.65, PROCESS_LOCAL, 1452 bytes)\n16/07/21 17:20:05 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.142.65:44061 with 1060.3 MB RAM, BlockManagerId(0, 172.19.142.65, 44061)\n16/07/21 17:20:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.142.65:44061 (size: 2.2 KB, free: 1060.3 MB)\n16/07/21 17:20:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.142.65:44061 (size: 3.5 KB, free: 1060.3 MB)\n16/07/21 17:20:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1117 ms on 172.19.142.65 (1/2)\n16/07/21 17:20:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1137 ms on 172.19.142.65 (2/2)\n16/07/21 17:20:06 INFO DAGScheduler: ShuffleMapStage 0 (map at test.scala:14) finished in 1.534 s\n16/07/21 17:20:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n16/07/21 17:20:06 INFO DAGScheduler: looking for newly runnable stages\n16/07/21 17:20:06 INFO DAGScheduler: running: Set()\n16/07/21 17:20:06 INFO DAGScheduler: waiting: Set(ResultStage 1)\n16/07/21 17:20:06 INFO DAGScheduler: failed: Set()\n16/07/21 17:20:06 INFO DAGScheduler: Missing parents for ResultStage 1: List()\n16/07/21 17:20:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:14), which is now runnable\n16/07/21 17:20:06 INFO MemoryStore: ensureFreeSpace(21216) called with curMem=43305, maxMem=278302556\n16/07/21 17:20:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.7 KB, free 265.3 MB)\n16/07/21 17:20:06 INFO MemoryStore: ensureFreeSpace(7035) called with curMem=64521, maxMem=278302556\n16/07/21 17:20:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 265.3 MB)\n16/07/21 17:20:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.142.65:35167 (size: 6.9 KB, free: 265.4 MB)\n16/07/21 17:20:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874\n16/07/21 17:20:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:14)\n16/07/21 17:20:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks\n16/07/21 17:20:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 172.19.142.65, PROCESS_LOCAL, 1217 bytes)\n16/07/21 17:20:06 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 172.19.142.65, PROCESS_LOCAL, 1217 bytes)\n16/07/21 17:20:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.142.65:44061 (size: 6.9 KB, free: 1060.3 MB)\n16/07/21 17:20:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.142.65:44229\n16/07/21 17:20:07 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 153 bytes\n16/07/21 17:20:07 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 774 ms on 172.19.142.65 (1/2)\n16/07/21 17:20:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 864 ms on 172.19.142.65 (2/2)\n16/07/21 17:20:07 INFO DAGScheduler: ResultStage 1 (saveAsTextFile at test.scala:14) finished in 0.864 s\n16/07/21 17:20:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n16/07/21 17:20:07 INFO DAGScheduler: Job 0 finished: saveAsTextFile at test.scala:14, took 2.455549 s\n16/07/21 17:20:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.142.65:35167 in memory (size: 6.9 KB, free: 265.4 MB)\n16/07/21 17:20:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.142.65:44061 in memory (size: 6.9 KB, free: 1060.3 MB)\n16/07/21 17:20:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.142.65:35167 in memory (size: 2.2 KB, free: 265.4 MB)\n16/07/21 17:20:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.142.65:44061 in memory (size: 2.2 KB, free: 1060.3 MB)\n16/07/21 17:20:08 INFO ContextCleaner: Cleaned shuffle 0\n16/07/21 17:20:08 INFO SparkUI: Stopped Spark web UI at http://172.19.142.65:4040\n16/07/21 17:20:08 INFO DAGScheduler: Stopping DAGScheduler\n16/07/21 17:20:08 INFO SparkDeploySchedulerBackend: Shutting down all executors\n16/07/21 17:20:08 INFO SparkDeploySchedulerBackend: Asking each executor to shut down\n16/07/21 17:20:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n16/07/21 17:20:08 INFO Utils: path = /tmp/spark-059b03ed-1a07-46ed-8020-8210b775fd88/blockmgr-86cf89c8-e6e1-4790-b7e9-b73689551a99, already present as root for deletion.\n16/07/21 17:20:08 INFO MemoryStore: MemoryStore cleared\n16/07/21 17:20:08 INFO BlockManager: BlockManager stopped\n16/07/21 17:20:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n16/07/21 17:20:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n16/07/21 17:20:08 INFO SparkContext: Successfully stopped SparkContext\n16/07/21 17:20:08 INFO Utils: Shutdown hook called\n16/07/21 17:20:08 INFO Utils: Deleting directory /tmp/spark-059b03ed-1a07-46ed-8020-8210b775fd88\n16/07/21 17:20:08 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.\n16/07/21 17:20:08 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.\n16/07/21 17:20:08 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.\n','2016-07-21 17:20:02','2016-07-21 17:20:08'),(131,'app-20160721172035-0014',3,'/home/a.jar','/home/A/text.txt','/home/003','test',2,3,1,'Using Spark\'s default log4j profile: org/apache/spark/log4j-defaults.properties\n16/07/21 17:20:33 INFO SparkContext: Running Spark version 1.4.0\n16/07/21 17:20:34 WARN Utils: Your hostname, linux resolves to a loopback address: 127.0.0.1; using 172.19.142.65 instead (on interface eno1)\n16/07/21 17:20:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n16/07/21 17:20:34 INFO SecurityManager: Changing view acls to: Hash.meng\n16/07/21 17:20:34 INFO SecurityManager: Changing modify acls to: Hash.meng\n16/07/21 17:20:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(Hash.meng); users with modify permissions: Set(Hash.meng)\n16/07/21 17:20:34 INFO Slf4jLogger: Slf4jLogger started\n16/07/21 17:20:34 INFO Remoting: Starting remoting\n16/07/21 17:20:34 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@172.19.142.65:37409]\n16/07/21 17:20:34 INFO Utils: Successfully started service \'sparkDriver\' on port 37409.\n16/07/21 17:20:34 INFO SparkEnv: Registering MapOutputTracker\n16/07/21 17:20:34 INFO SparkEnv: Registering BlockManagerMaster\n16/07/21 17:20:34 INFO DiskBlockManager: Created local directory at /tmp/spark-941ef3bd-648c-4bc5-ae8c-d3bb17872e35/blockmgr-29757993-ff92-4443-a1b9-1bcc3621eb1f\n16/07/21 17:20:34 INFO MemoryStore: MemoryStore started with capacity 265.4 MB\n16/07/21 17:20:34 INFO HttpFileServer: HTTP File server directory is /tmp/spark-941ef3bd-648c-4bc5-ae8c-d3bb17872e35/httpd-8932036d-9d16-4ef8-9dc5-1a4c6a62c861\n16/07/21 17:20:34 INFO HttpServer: Starting HTTP Server\n16/07/21 17:20:34 INFO Utils: Successfully started service \'HTTP file server\' on port 40307.\n16/07/21 17:20:34 INFO SparkEnv: Registering OutputCommitCoordinator\n16/07/21 17:20:34 INFO Utils: Successfully started service \'SparkUI\' on port 4040.\n16/07/21 17:20:34 INFO SparkUI: Started SparkUI at http://172.19.142.65:4040\n16/07/21 17:20:34 INFO SparkContext: Added JAR file:/home/Hash.meng/IdeaProjects/javahdfs/web/LocalFile/null-a.jar at http://172.19.142.65:40307/jars/null-a.jar with timestamp 1469092834892\n16/07/21 17:20:34 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@172.19.142.65:7077/user/Master...\n16/07/21 17:20:35 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20160721172035-0014\n16/07/21 17:20:35 INFO AppClient$ClientActor: Executor added: app-20160721172035-0014/0 on worker-20160720192555-172.19.142.65-36703 (172.19.142.65:36703) with 3 cores\n16/07/21 17:20:35 INFO SparkDeploySchedulerBackend: Granted executor ID app-20160721172035-0014/0 on hostPort 172.19.142.65:36703 with 3 cores, 2.0 GB RAM\n16/07/21 17:20:35 INFO AppClient$ClientActor: Executor updated: app-20160721172035-0014/0 is now LOADING\n16/07/21 17:20:35 INFO AppClient$ClientActor: Executor updated: app-20160721172035-0014/0 is now RUNNING\n16/07/21 17:20:35 INFO Utils: Successfully started service \'org.apache.spark.network.netty.NettyBlockTransferService\' on port 36607.\n16/07/21 17:20:35 INFO NettyBlockTransferService: Server created on 36607\n16/07/21 17:20:35 INFO BlockManagerMaster: Trying to register BlockManager\n16/07/21 17:20:35 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.142.65:36607 with 265.4 MB RAM, BlockManagerId(driver, 172.19.142.65, 36607)\n16/07/21 17:20:35 INFO BlockManagerMaster: Registered BlockManager\n16/07/21 17:20:35 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n16/07/21 17:20:35 INFO MemoryStore: ensureFreeSpace(33392) called with curMem=0, maxMem=278302556\n16/07/21 17:20:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.6 KB, free 265.4 MB)\n16/07/21 17:20:35 INFO MemoryStore: ensureFreeSpace(3631) called with curMem=33392, maxMem=278302556\n16/07/21 17:20:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KB, free 265.4 MB)\n16/07/21 17:20:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.142.65:36607 (size: 3.5 KB, free: 265.4 MB)\n16/07/21 17:20:35 INFO SparkContext: Created broadcast 0 from textFile at test.scala:13\n16/07/21 17:20:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n16/07/21 17:20:35 WARN LoadSnappy: Snappy native library not loaded\n16/07/21 17:20:35 INFO FileInputFormat: Total input paths to process : 1\nException in thread \"main\" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://127.0.0.1:9000/home/003 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1065)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:989)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:965)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:897)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:896)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1400)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1379)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1379)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:286)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1379)\n\tat test$.main(test.scala:14)\n\tat test.main(test.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n16/07/21 17:20:35 INFO SparkContext: Invoking stop() from shutdown hook\n16/07/21 17:20:35 INFO SparkUI: Stopped Spark web UI at http://172.19.142.65:4040\n16/07/21 17:20:35 INFO DAGScheduler: Stopping DAGScheduler\n16/07/21 17:20:35 INFO SparkDeploySchedulerBackend: Shutting down all executors\n16/07/21 17:20:35 INFO SparkDeploySchedulerBackend: Asking each executor to shut down\n16/07/21 17:20:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n16/07/21 17:20:35 INFO Utils: path = /tmp/spark-941ef3bd-648c-4bc5-ae8c-d3bb17872e35/blockmgr-29757993-ff92-4443-a1b9-1bcc3621eb1f, already present as root for deletion.\n16/07/21 17:20:35 INFO MemoryStore: MemoryStore cleared\n16/07/21 17:20:35 INFO BlockManager: BlockManager stopped\n16/07/21 17:20:35 INFO BlockManagerMaster: BlockManagerMaster stopped\n16/07/21 17:20:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n16/07/21 17:20:35 INFO SparkContext: Successfully stopped SparkContext\n16/07/21 17:20:35 INFO Utils: Shutdown hook called\n16/07/21 17:20:35 INFO Utils: Deleting directory /tmp/spark-941ef3bd-648c-4bc5-ae8c-d3bb17872e35\n','2016-07-21 17:20:33','2016-07-21 17:20:36');

#
# Structure for table "note"
#

DROP TABLE IF EXISTS `note`;
CREATE TABLE `note` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(128) DEFAULT NULL,
  `time` varchar(64) DEFAULT NULL,
  `author` varchar(64) DEFAULT NULL,
  `content` text,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;

#
# Data for table "note"
#

INSERT INTO `note` VALUES (2,'登陆设置状态改变','2016/06/12 17:01:37','system','当前系统已经设置为禁止登陆！'),(3,'登陆设置状态改变','2016/07/12 17:15:42','system','当前系统已经设置为允许登陆！'),(4,'登陆设置状态改变','2016/07/12 17:15:46','system','当前系统已经设置为禁止登陆！'),(5,'任务设置状态改变','2016/07/12 17:15:49','system','当前系统已经设置为禁止提交任务！'),(6,'任务设置状态改变','2016/07/12 17:15:53','system','当前系统已经设置为允许提交任务！'),(7,'任务设置状态改变','2016/07/12 17:15:56','system','当前系统已经设置为禁止提交任务！'),(8,'登陆设置状态改变','2016/07/12 17:16:06','system','当前系统已经设置为允许登陆！'),(9,'任务设置状态改变','2016/07/12 17:16:11','system','当前系统已经设置为允许提交任务！'),(13,'登陆设置状态改变','2016/07/12 18:18:20','system','当前系统已经设置为禁止登陆！'),(14,'登陆设置状态改变','2016/07/12 19:51:36','system','当前系统已经设置为允许登陆！'),(15,'任务设置状态改变','2016/07/12 22:38:07','system','当前系统已经设置为禁止提交任务！'),(16,'任务设置状态改变','2016/07/14 14:24:47','system','当前系统已经设置为允许提交任务！');

#
# Structure for table "sys"
#

DROP TABLE IF EXISTS `sys`;
CREATE TABLE `sys` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `login_state` int(4) DEFAULT '1',
  `job_state` int(4) DEFAULT '1',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8 COMMENT='系统设置情况';

#
# Data for table "sys"
#

INSERT INTO `sys` VALUES (1,1,1);

#
# Structure for table "user"
#

DROP TABLE IF EXISTS `user`;
CREATE TABLE `user` (
  `id` int(10) NOT NULL AUTO_INCREMENT,
  `username` varchar(30) NOT NULL,
  `password` varchar(64) NOT NULL,
  `nickname` varchar(30) DEFAULT NULL,
  `email` varchar(255) DEFAULT NULL,
  `avatar` varchar(255) DEFAULT NULL,
  `thumbnail` varchar(255) DEFAULT NULL,
  `last_login_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;

#
# Data for table "user"
#

INSERT INTO `user` VALUES (3,'E41414068','E10ADC3949BA59ABBE56E057F20F883E','123','test@qq.com','/upload/avatar/20160719063844.jpg','/upload/avatar/thumbnail20160719063844.jpg','2016-07-21 15:36:44');
